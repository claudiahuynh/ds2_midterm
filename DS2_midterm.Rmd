---
title: "DS2_midterm"
author: "My An Huynh, Soomin You"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r message = FALSE}
library(tidyverse)
library(MASS)
library(caret)
library(corrplot)
library(mgcv)
library(patchwork)
library(ggplot2)
library(gtsummary)
```

## Load Data
```{r}
load("data/dat1.RData")
load("data/dat2.RData")
```

## Train Data Summary Statistics 
```{r train_data}
train = dat1 |>
  dplyr::select(-id)

x = model.matrix(log_antibody ~ ., data = train)[, -1]
y = train$log_antibody

tbl_summary(train)
```

First, the train data was loaded and separated into predictor variable matrix and response variable vector. A summary statistics table was made to summarize the mean and proportions of the variables in the train data. 

\newpage

The following is a brief explanation of the predictor variables and the response variable summarized in the table above. 

* race (1 = White, 2 = Asian, 3 = Black, 4 = Hispanic)
* gender (0 = Female, 1 = Male)
* smoking levels (0 = non-smoker, 1 = former smoker, 2 = current smoker)
* hypertension (0 = no hypertension, 1 = hypertension)
* diabetes (0 = non-diabetic, 1 = diabetic)
* SBP (systolic blood pressure in mmHg)
* LDL (LDL cholesterol in mg/dL)
* age (years)
* height (cm)
* weight (kg)
* BMI (kg/m^2) 
* time (time passed since vaccination in days). 
* log_antibody (log transformed antibody level) 


## Train Data Exploratory Data Analysis 
```{r eda}
theme1 = trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(.8, .1, .1, 1)
theme1$plot.line$lwd = 2
theme1$strip.background$col = rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

featurePlot(x = train[, -c(2,3,4,8,9,13)], 
            y = train$log_antibody, 
            plot = "scatter", 
            span = 0.5,
            type = c("p", "smooth"), 
            labels = c("Predictors", "Log_antibody"))

# boxplot for factor/binary variables 
p1 = ggplot(aes(x = factor(gender), y = log_antibody), data = train) + 
  geom_boxplot() 
p2 = ggplot(aes(x = factor(race), y = log_antibody), data = train) + 
  geom_boxplot()
p3 = ggplot(aes(x = factor(diabetes), y = log_antibody), data = train) + 
  geom_boxplot()
p4 = ggplot(aes(x = factor(hypertension), y = log_antibody), data = train) + 
  geom_boxplot()
p5 = ggplot(aes(x = factor(smoking), y = log_antibody), data = train) + 
  geom_boxplot()
p1 + p2 + p3 + p4 + p5

corrplot(cor(x), method = "circle", type = "full")
corrplot(cor(x), method = "number", type = "full")
```

Then, the visualization of the train data was checked. Scatterplots were made for the numeric predictor variables and boxplots were made for the factor or binary variables. 

According to the scatterplots for `Systolic blood pressure (SBP)`, `LDL cholesterol (LDL)`, `Time since vaccination (time)`,  `Age (age)`, `Height (height)`, `Weight (weight)`, `BMI (bmi)`, these variables indicate relatively linear relationship with the response variable `Log-transformed antibody level (log_antibody)`. Also, the boxplots indicate that the the factor and binary predictors show Gaussian characteristic. 

Correlation plot was also checked for the train data and no strong multicolinearity was observed. 


## Test Data 
Then, the exploratory analysis was performed for the independent test data that has the same structure as the train data. 

```{r test_data}
test = dat2 |> 
  dplyr::select(-id)

x_test = model.matrix(log_antibody ~ ., data = test)[, -1]
y_test = test$log_antibody

tbl_summary(test)
```

\newpage

```{r}
featurePlot(x = test[, -c(2,3,4,8,9,13)], 
            y = y_test, 
            plot = "scatter", 
            span = 0.5,
            type = c("p", "smooth"), 
            labels = c("Predictors", "Log_antibody"))

p1_test = ggplot(aes(x = factor(gender), y = log_antibody), data = test) + 
  geom_boxplot() 
p2_test = ggplot(aes(x = factor(race), y = log_antibody), data = test) + 
  geom_boxplot()
p3_test = ggplot(aes(x = factor(diabetes), y = log_antibody), data = test) + 
  geom_boxplot()
p4_test = ggplot(aes(x = factor(hypertension), y = log_antibody), data = test) + 
  geom_boxplot()
p5_test = ggplot(aes(x = factor(smoking), y = log_antibody), data = test) + 
  geom_boxplot()
p1_test + p2_test + p3_test + p4_test + p5_test

corrplot(cor(x_test), method = "circle", type = "full")
```

According to the summary statistics and visualization plots, it was be confirmed that the structure of the test data is similar to that of the train data. 


## Model training 
To find an optimal prediction model of antibody levels that show the impact of the demographic and clinical factors, we decided to build various models and compare. We used linear, elastic net, PLS, MARS and GAM models as shown below. 

```{r linear}
#linear
ctrl1 = trainControl(method = "cv", number = 10)

set.seed(1)
linear_mod = train(x, y,
                   method = "lm", 
                   trControl = ctrl1)
summary(linear_mod)

# test error 
lm_pred = predict(linear_mod, newdata = x_test)
lm_rmse = sqrt(mean((y_test - lm_pred)^2))

cat("Test error for the linear model: ", lm_rmse, "\n")
```


```{r elastic net}
# elastic net 
set.seed(1)
enet_mod = train(x = x, 
                 y = y, 
                 method = "glmnet", 
                 tuneGrid = expand.grid(alpha = seq(0, 1, length = 25), 
                                        lambda = exp(seq(-2, 2, length = 100))),
                 trControl = ctrl1)

best_alpha = enet_mod$bestTune$alpha
best_lambda = enet_mod$bestTune$lambda

# test error 
enet_pred = predict(enet_mod, newdata = x_test)
enet_rmse = sqrt(mean((y_test - enet_pred)^2))

cat("The optimal alpha for the elastic net model: ", best_alpha, "\n")
cat("The optimal alpha for the elastic net model: ", best_lambda, "\n")
cat("Test error for the linear model: ", enet_rmse, "\n")
```


```{r pls}
# pls model
set.seed(1)
pls_mod = train(x = x,
                y = y, 
                method = "pls", 
                tuneGrid = data.frame(ncomp = 1:15), 
                trControl = ctrl1, 
                preProcess = c("center", "scale"))
summary(pls_mod)

# test error 
pls_pred = predict(pls_mod, newdata = x_test)
pls_rmse = sqrt(mean((y_test - pls_pred)^2))

cat("Test error for the linear model: ", pls_rmse, "\n")

ggplot(pls_mod, highlight = TRUE)
```


```{r}
# mars
set.seed(1)
mars_mod = train(x = x, 
                 y = y, 
                 method = "earth", 
                 trControl = ctrl1)

summary(mars_mod)

# test error 
mars_pred = predict(mars_mod, newdata = x_test)
mars_rmse = sqrt(mean((y_test - mars_pred)^2))

cat("Test error for the linear model: ", mars_rmse, "\n")

pdp::partial(mars_mod, pred.var = c("bmi"), grid.resolution = 10) |>
  autoplot()
```

To better understand the relationship, partial dependence plot (PDP) for BMI predictor variable and the response variable was plotted. 

```{r}
# gam  
set.seed(1)
gam_mod = train(x = x, 
                y = y, 
                method = "gam", 
                trControl = ctrl1)

gam_mod$bestTune
gam_mod$finalModel

summary(gam_mod)

# test error 
gam_pred = predict(gam_mod, newdata = x_test)
gam_rmse = sqrt(mean((y_test - gam_pred)^2))

cat("Test error for the linear model: ", gam_rmse, "\n")
```


## Model validation 
To evaluate the robustness and generalizability of the prediction model using the new dataset, test data, the five fitted models were compared using resampling. 

```{r}
set.seed(1)
resamp = resamples(list(lm = linear_mod, 
                        enet = enet_mod, 
                        pls = pls_mod,                         
                        MARS = mars_mod, 
                        GAM = gam_mod))
summary(resamp)
bwplot(resamp, metric = "RMSE")
```

According to the model comparison via 10-fold cross validation, the mean RMSE of the MARS model is the lowest for this specific dataset and predefined seed. Hence, MARS is chosen as the final model to predict the relationship between log-transformed antibody levels and the predictors of interest. Also, MARS offers better interpretability. 


