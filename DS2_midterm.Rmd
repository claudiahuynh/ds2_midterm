---
title: "DS2_midterm"
author: "My An Huynh, Soomin You"
date: "`r Sys.Date()`"
output: github_document
---

```{r}
library(tidyverse)
library(MASS)
library(caret)
library(corrplot)
library(mgcv)
library(patchwork)
```

## Load Data
```{r}
load("data/dat1.RData")
load("data/dat2.RData")
```

## Summary Statistics 
## Exploratory Data Analysis 
```{r eda}
#train data 
train = dat1 |>
  dplyr::select(-id)

x = model.matrix(log_antibody ~ ., data = train)[, -1]
y = train$log_antibody

#test data 
test = dat2 |> 
  dplyr::select(-id)

x_test = model.matrix(log_antibody ~ ., data = test)[, -1]
y_test = test$log_antibody

# visualization of the train data 
featurePlot(x = train[, -c(2,3,4,8,9, 13)], 
            y = train$log_antibody, 
            plot = "scatter", 
            labels = c("Predictors", "Log_antibody"))

# boxplot for factor/binary variables 
p1 = ggplot(aes(x = factor(gender), y = log_antibody), data = train) + 
  geom_boxplot()
p2 = ggplot(aes(x = factor(race), y = log_antibody), data = train) + 
  geom_boxplot()
p3 = ggplot(aes(x = factor(race), y = log_antibody), data = train) + 
  geom_boxplot()
p4 = ggplot(aes(x = factor(diabetes), y = log_antibody), data = train) + 
  geom_boxplot()
p5 = ggplot(aes(x = factor(hypertension), y = log_antibody), data = train) + 
  geom_boxplot()
p6 = ggplot(aes(x = factor(smoking), y = log_antibody), data = train) + 
  geom_boxplot()
p1 + p2 + p3 + p4 + p5 + p6

corrplot(cor(x), method = "circle", type = "full")
```

Both the train data and the test data were loaded and separated into response variable and predictor variable matrices. Scatterplots were made for the numeric predictor variables and boxplots were made for the factor or binary variables. 

Correlation plot was also checked and no multicolinearity was observed. 


## Model training 
To find an optimal prediction model of antibody levels that show the impact of the demographic and clinical factors, we decide to build various models and compare. We use elastic net, MARS, GAM, PCR and PLS models as shown below. 

```{r linear}
#linear
ctrl1 = trainControl(method = "cv", number = 10)

set.seed(1)
linear_mod = train(x, y,
                   method = "lm", 
                   trControl = ctrl1)
summary(linear_mod)

# test error 
pred_lm = predict(linear_mod, newdata = x_test)
mean((y_test - pred_lm)^2)
```


```{r elastic net}
# elastic net 
set.seed(1)
enet_mod = train(x = x, 
                 y = y, 
                 method = "glmnet", 
                 tuneGrid = expand.grid(alpha = seq(0, 1, length = 25), 
                                        lambda = exp(seq(-2, 2, length = 100))),
                 trControl = ctrl1)

enet_mod$bestTune
```

Maybe describe how we determined the tuning grid range for lambda...? 


```{r pcr}
# pcr model 
pcr_mod = train(x = x,
                y = y, 
                method = "pcr", 
                tuneGrid = data.frame(ncomp = 1:15), 
                trControl = ctrl1, 
                preProcess = c("center", "scale"))
summary(pcr_mod)
```

```{r pls}
set.seed(1)
# pls model 
pls_mod = train(x = x,
                y = y, 
                method = "pls", 
                tuneGrid = data.frame(ncomp = 1:15), 
                trControl = ctrl1, 
                preProcess = c("center", "scale"))
summary(pls_mod)

# test error 
pred_pls = predict(pls_mod, newdata = x_test)
mean((y_test - pred_pls)^2)

ggplot(pls_mod, highlight = TRUE)
```

```{r}
# mars
set.seed(1)
mars_mod = train(x = x, 
                 y = y, 
                 method = "earth", 
                 trControl = ctrl1)

summary(mars_mod)

mars_pred = predict(mars_mod, newdata = x_test)
mars_rmse = sqrt(mean((y_test - mars_pred)^2))
```

```{r}
# gam  
set.seed(1)
gam_mod = train(x = x, 
                y = y, 
                method = "gam", 
                trControl = ctrl1)

gam_mod$bestTune
gam_mod$finalModel

summary(gam_mod)
```



## Model validation 
6) comparison 
```{r}
set.seed(1)
resamp = resamples(list(lm = linear_mod, 
                        enet = enet_mod, 
                        pcr = pcr_mod, 
                        pls = pls_mod,                         
                        MARS = mars_mod, 
                        GAM = gam_mod))
summary(resamp)
bwplot(resamp, metric = "RMSE")
```


Generalization 
```{r}
mars.pred = predict(mars_mod, newdata = test)
pcr.pred = predict(pcr_mod,newdata = test)
lda.pred <- predict(gam_mod, newdata = test)

```

## Conclusion 

our best model is MARS since it has the lowest mean RMSE value. 